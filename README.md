# ğŸš€ LucenAI â€“ Sentiment Analysis on Crypto Tweets

## ğŸ” Objective

This project aims to analyze **Bitcoin (BTC)** sentiment using public Twitter data. It leverages pre-trained NLP modelsâ€”especially **DistilBERT**â€”through a full pipeline from preprocessing to deployment via FastAPI.

---

## ğŸ§© Key Pipeline Stages

- ğŸ“¥ **Data ingestion and cleaning**
- ğŸ”§ **Fine-tuning** of `DistilBERT` for binary sentiment classification (positive/negative)
- ğŸ“ˆ **Imbalance handling**, overfitting prevention, and ambiguous cases filtering
- ğŸ§ª **Evaluation** on a dedicated test set
- ğŸŒ **Deployment** of a REST API with FastAPI

---

## ğŸ“ Project Structure

```bash
lucen-ai/
â”œâ”€â”€ models/                            # ğŸ§  Fine-tuned models (saved checkpoints)
â”‚   â””â”€â”€ distilbert_sentiment/
â”‚       â”œâ”€â”€ config.json
â”‚       â”œâ”€â”€ tokenizer/
â”‚       â””â”€â”€ tf_model.h5
â”œâ”€â”€ data/                              # ğŸ“Š Dataset (raw)
â”‚   â””â”€â”€ BTC_Tweets_Sentiments.csv
â”œâ”€â”€ notebooks/                         # ğŸ““ Jupyter notebooks (EDA & experimentation)
â”‚   â”œâ”€â”€ data/                          # Intermediate files used during experimentation
â”‚   â”œâ”€â”€ logs/                          # Training logs (TensorBoard, etc.)
â”‚   â”œâ”€â”€ models/                        # Temporary model outputs
â”‚   â”œâ”€â”€ preparation_dataset.ipynb      # Data cleaning and preparation
â”‚   â””â”€â”€ BERTweet_vs_distilBERT.ipynb   # Model comparison notebook
â”œâ”€â”€ lucenai/                           # ğŸ§  Core Python package
â”‚   â”œâ”€â”€ train/                         # ğŸ”§ Data pipeline and model logic
â”‚   â”‚   â”œâ”€â”€ preprocess.py              # Text preprocessing
â”‚   â”‚   â”œâ”€â”€ tokenizer.py               # Tokenizer loading/wrapping
â”‚   â”‚   â”œâ”€â”€ model.py                   # Model architecture & compilation
â”‚   â”‚   â”œâ”€â”€ test.py                    # Evaluation functions
â”‚   â”‚   â””â”€â”€ utils.py                   # Utility functions
â”‚   â”œâ”€â”€ api/                           # ğŸš€ FastAPI application
â”‚   â”‚   â”œâ”€â”€ predict.py                 # Inference endpoint
â”‚   â”‚   â””â”€â”€ schemas.py                 # Pydantic schemas
â”‚   â”œâ”€â”€ config/                        # âš™ï¸ Project configuration
â”‚   â”‚   â””â”€â”€ settings.py
â”‚   â””â”€â”€ frontend/                      # ğŸŒ Web frontend (basic UI)
â”‚       â”œâ”€â”€ index.html
â”‚       â”œâ”€â”€ app.js
â”‚       â”œâ”€â”€ crypta_data.json
â”‚       â””â”€â”€ style.css
â”œâ”€â”€ scripts/                           # ğŸ› ï¸ CLI entrypoints
â”‚   â”œâ”€â”€ train.py                       # Launch model training
â”‚   â””â”€â”€ app.py                         # Launch FastAPI server
â”œâ”€â”€ README.md                          # ğŸ“– Project overview and instructions
â”œâ”€â”€ Docker.train                       # Dockerfile for training environment
â”œâ”€â”€ Docker.api                         # Dockerfile for API environment
â”œâ”€â”€ requirements_CPU.txt               # Minimal dependencies
â”œâ”€â”€ requirements_GPU.txt               # Dev dependencies for GPU runs
â”œâ”€â”€ .gitignore
â””â”€â”€ LICENSE
```

---

## ğŸ“’ Jupyter Notebooks

### ğŸ§© Key Pipeline Stages
- ğŸ“¥ **Data ingestion and cleaning**
- âš–ï¸ **Model performance comparison**: `BERTweet` vs `DistilBERT`

The `notebooks/` directory includes exploratory and comparative notebooks:

- `preparation_dataset.ipynb`: Text cleaning, EDA, statistics
- `BERTweet_vs_distilBERT.ipynb`: Model benchmarking
- `notebooks/models/`: Model checkpoints generated during experimentation
- `notebooks/data/` : cleaned dataset for training, validation and test

### ğŸ“ˆ Using TensorBoard for Training Visualization

To visualize training progress (loss, accuracy, learning curves), we use **TensorBoard** via a Keras callback.

To launch TensorBoard in your terminal:
```bash
tensorboard --logdir notebooks/logs
```

> Logs are automatically saved in `notebooks/logs/` with a timestamped subdirectory.

---

## ğŸ§  Pretrained Model

The integrated model is a fine-tuned `distilbert-base-uncased` for binary sentiment classification. It is stored under:

```
models/distilbert_sentiment/
â”œâ”€â”€ config.json
â”œâ”€â”€ tokenizer/
â””â”€â”€ tf_model.h5
```

---

## ğŸŒ API Usage (FastAPI)

The FastAPI server is containerized and exposes sentiment prediction as a REST endpoint.

### ğŸ” Example `POST /predict` request

```bash
curl -X POST http://localhost:8000/predict \
     -H "Content-Type: application/json" \
     -d '{"text": "Bitcoin is going to the moon ğŸš€"}'
```

Response:
```json
{
  "label": "positive",
  "confidence": 0.987
}
```

### ğŸ” API health check (`GET /health`)

```bash
curl http://localhost:8000/health
```

Response:
```json
{
  "status": "ok"
}
```

---

## ğŸ³ Docker

This project uses two separate Dockerfiles to manage training and deployment environments independently:

- `Docker.train`: for training the DistilBERT model on GPU (L4, Lightning AI-compatible)
- `Docker.api`: for serving the FastAPI + frontend on CPU

---

### ğŸ³ 1. Build Docker Images

From the project root:

```bash
# Build training image (GPU)
docker build -f Dockerfile.train -t lucen-ia-train .

# Build API image (CPU)
docker build -f Docker.api -t lucen-api .
```

---

### ğŸ“ 2. Run Training (GPU-enabled)

To train the model on GPU using Docker, execute the following command from the project root:

```bash
docker run --rm --gpus all \
  -v $(pwd)/lucenai/models:/app/lucenai/models \
  -v $(pwd)/data:/app/data \
  -v $(pwd)/scripts:/app/scripts \
  -v $(pwd)/notebooks:/app/notebooks \
  -v $(pwd)/lucenai/config:/app/lucenai/config \
  lucen-ia-train
```

This command will:

- Load the BTC tweet dataset from `/app/data`
- Preprocess and tokenize the tweets using DistilBERT
- Fine-tune the sentiment classification model
- Saves the trained model (`model.keras`), configuration (`config.json`), and tokenizer files to:

```bash
lucenai/models/distilbert_sentiment/
â”œâ”€â”€ config.json
â”œâ”€â”€ model.keras
â””â”€â”€ tokenizer/
    â”œâ”€â”€ vocab.txt
    â”œâ”€â”€ tokenizer_config.json
    â””â”€â”€ special_tokens_map.json
```

---

### ğŸŒ 3. Serve the API (CPU)

```bash
docker run -p 8000:8000 lucen-api
```

Then visit:

- ğŸ–¥ï¸ Frontend: [http://localhost:8000](http://localhost:8000)
- ğŸ“š Swagger Docs: [http://localhost:8000/docs](http://localhost:8000/docs)

---

## ğŸ“š Libraries Used

This project is built upon a robust and modern machine learning stack:

| Library | Description |
|--------|-------------|
| ğŸ¤— [Transformers](https://huggingface.co/docs/transformers) | Pretrained NLP models and tokenizers (e.g., DistilBERT) |
| ğŸ§  [TensorFlow](https://www.tensorflow.org/) | Deep learning framework for model fine-tuning and inference |
| ğŸ“Š [Scikit-learn](https://scikit-learn.org/) | Utilities for splitting, metrics, and preprocessing |
| ğŸ [Pandas](https://pandas.pydata.org/) | Data handling and manipulation of tweet datasets |
| ğŸ“ˆ [Matplotlib](https://matplotlib.org/) & [Seaborn](https://seaborn.pydata.org/) | Visualization of model performance and data distributions |
| ğŸŒ [FastAPI](https://fastapi.tiangolo.com/) | High-performance web framework for exposing the model as an API |
| ğŸš€ [Uvicorn](https://www.uvicorn.org/) | ASGI server to run the FastAPI backend |

---

## ğŸ“¬ Contact

- **Author**: Anthony Morin  
- **License**: MIT

---

Â© 2025 â€“ Anthony Morin. All code is open-sourced under the MIT License.